{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 15:52:50.590969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 15:52:54.607249: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/shared/CMPT/scratch/mka219/.conda/envs/image_segmentation/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/lib64:\n",
      "2023-04-07 15:52:54.607888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/shared/CMPT/scratch/mka219/.conda/envs/image_segmentation/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/lib64:\n",
      "2023-04-07 15:52:54.607897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pyrender\n",
    "import trimesh\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import Emotion\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import face_alignment_v1\n",
    "fa = face_alignment_v1.FaceAlignment(face_alignment_v1.LandmarksType._3D, flip_input=False)\n",
    "fa_2d = face_alignment_v1.FaceAlignment(face_alignment_v1.LandmarksType._2D, flip_input=False)\n",
    "#import Age\n",
    "opencv_home = cv2.__file__\n",
    "folders = opencv_home.split(os.path.sep)[0:-1]\n",
    "opencv_path = folders[0]\n",
    "for folder in folders[1:]:\n",
    "    opencv_path = opencv_path + \"/\" + folder\n",
    "import tensorflow as tf\n",
    "tf_version = tf.__version__\n",
    "tf_major_version = int(tf_version.split(\".\", maxsplit=1)[0])\n",
    "tf_minor_version = int(tf_version.split(\".\")[1])\n",
    "if tf_major_version == 1:\n",
    "    from keras.preprocessing import image as k_image\n",
    "elif tf_major_version == 2:\n",
    "    from tensorflow.keras.preprocessing import image as k_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MEDIAPIPE WITH PYRENDER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glTF file\n",
    "#model_trimesh = trimesh.load('/home/mka219/sfuhome/ar_project/data/ownfilter_without_v2.glb')\n",
    "model_trimesh = trimesh.load('/home/mka219/sfuhome/ar_project/data/own_without_man.glb')\n",
    "width = 1080\n",
    "height = 720\n",
    "model_meshes = []\n",
    "vertices = []\n",
    "faces = []\n",
    "for geometry in model_trimesh.geometry.values():\n",
    "    if isinstance(geometry, trimesh.Trimesh):\n",
    "        model_meshes.append(geometry)\n",
    "        vertices.append(geometry.vertices)\n",
    "        faces.append(geometry.faces)\n",
    "# Convert to pyrender Mesh object\n",
    "\n",
    "\n",
    "model_mesh = pyrender.Mesh.from_trimesh(model_meshes)\n",
    "\n",
    "scene = pyrender.Scene()\n",
    "scene.add(model_mesh)\n",
    "\n",
    "camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1)\n",
    "camera_pose = np.eye(4)\n",
    "\n",
    "camera_node = scene.add(camera, pose=camera_pose)\n",
    "\n",
    "point_light = pyrender.PointLight(color=[1.0, 1.0, 1.0], intensity=10.0)\n",
    "point_node = scene.add(point_light, pose=camera_pose)\n",
    "\n",
    "light = pyrender.SpotLight(color=[1.0, 1.0, 1.0], intensity=10.0, innerConeAngle=np.pi/16, outerConeAngle=np.pi/6)\n",
    "\n",
    "light_pose = np.eye(4)\n",
    "light_pose[:3, 3] = [0.0, 0.0, 1.0]\n",
    "light_node = scene.add(light, pose=camera_pose)\n",
    "\n",
    "direction = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=1.0)\n",
    "direction_node = scene.add(direction, pose=camera_pose)\n",
    "\n",
    "\n",
    "renderer = pyrender.OffscreenRenderer(viewport_width=300, viewport_height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2023-04-07 15:53:43.424271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 15:53:43.447404: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/shared/CMPT/scratch/mka219/.conda/envs/image_segmentation/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/lib64:\n",
      "2023-04-07 15:53:43.447424: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-07 15:53:43.447699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_tracking_confidence=0.1, min_detection_confidence=0.1)\n",
    "\n",
    "sunglasses = cv2.imread(\"/home/mka219/sfuhome/ar_project/data/sunglasses.png\", cv2.IMREAD_UNCHANGED)\n",
    "neutral_filter = cv2.imread(\"/home/mka219/sfuhome/ar_project/comedy-glasses.png\", cv2.IMREAD_UNCHANGED)\n",
    "face_detector=cv2.CascadeClassifier(cv2.data.haarcascades +\"/haarcascade_frontalface_default.xml\")\n",
    "emotion_detector = Emotion.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"/home/mka219/sfuhome/ar_project/input.mov\")\n",
    "# Define the camera intrinsics (focal length, principal point)\n",
    "fx = width / (2 * np.tan(np.radians(63.5 / 2)))  # Focal length in x-axis (pixels)\n",
    "fy = height / (2 * np.tan(np.radians(63.5 / 2)))\n",
    "cx = width / 2  # Image center x-coordinate\n",
    "cy = height / 2 \n",
    "\n",
    "camera_matrix = np.array([\n",
    "    [fx, 0, cx],\n",
    "    [0, fy, cy],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "model_vertices = np.array([\n",
    "            (1.672, -0.008797, -3.2535),     # mid eye\n",
    "            (1.914, -0.004085, -4.2511),   # chin\n",
    "            (1.6871, 0.924,  -2.3461), # Left eye corner\n",
    "            (-1.6901 , -0.93014,  -2.3184), # Right eye corner\n",
    "            (1.56 , 0.24,  -3.63),# Left mouth corner\n",
    "            (-1.56 , -0.25,  3.63)  # Right mouth corner\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "theta = np.radians(10)\n",
    "Rotate_camera = np.eye(4)\n",
    "Rotate_camera[:3, :3] = np.array([\n",
    "    [np.cos(theta), 0, np.sin(theta)],\n",
    "    [0, 1, 0],\n",
    "    [-np.sin(theta), 0, np.cos(theta)]\n",
    "])\n",
    "theta = np.radians(90)\n",
    "Rotate_camera = np.eye(4)\n",
    "Rotate_camera[:3, :3] = np.array([\n",
    "    [np.cos(theta), 0, np.sin(theta)],\n",
    "    [0, 1, 0],\n",
    "    [-np.sin(theta), 0, np.cos(theta)]\n",
    "])\n",
    "\n",
    "# Default camera distortion coefficients\n",
    "dist_coeffs = np.array([0, 0, 0, 0], dtype=np.float32)\n",
    "\n",
    "ind =0  \n",
    "with open('output.txt', 'w') as f:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        color_img = frame\n",
    "        gray_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = face_mesh.process(rgb_frame)\n",
    "\n",
    "        if result.multi_face_landmarks:\n",
    "            # Get the first detected face\n",
    "            face_landmarks = result.multi_face_landmarks[0]\n",
    "        else:\n",
    "            cv2.imshow('AR Filter', frame)\n",
    "            continue\n",
    "        \n",
    "        faces, _, scores = face_detector.detectMultiScale3( gray_img, scaleFactor=1.1, minNeighbors=10, outputRejectLevels=True )\n",
    "        if len(faces)==0:\n",
    "            cv2.imshow('AR Filter', frame)\n",
    "            continue\n",
    "        #cv2.imwrite('coloredFrame.jpg', color)\n",
    "        face = faces[0]\n",
    "        gray_cropped_image = gray_img[face[1]:face[1]+face[3], face[0]:face[0]+face[2]]\n",
    "        color_cropped_img = color_img[face[1]:face[1]+face[3], face[0]:face[0]+face[2]]\n",
    "        factor_0 = 224 / color_cropped_img.shape[0]\n",
    "        factor_1 = 224 / color_cropped_img.shape[1]\n",
    "        factor = min(factor_0, factor_1)\n",
    "        dsize = (int(color_cropped_img.shape[1] * factor), int(color_cropped_img.shape[0] * factor))\n",
    "        color_cropped_img = cv2.resize(color_cropped_img, (224,224))\n",
    "        img_pixels = k_image.img_to_array(color_cropped_img)  # what this line doing? must?\n",
    "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels /= 255 \n",
    "\n",
    "        img_gray = cv2.resize(gray_cropped_image, (48, 48))\n",
    "        img_gray = np.expand_dims(img_gray, axis=0)\n",
    "        # Emotion prediction\n",
    "        emotion_predictions = emotion_detector.predict(img_gray, verbose=0)[0]\n",
    "        emotion_predictions = Emotion.labels[np.argmax(emotion_predictions)]\n",
    "        \n",
    "        # Get the face landmarks\n",
    "\n",
    "        if emotion_predictions==\"happy\":\n",
    "            image_points = []\n",
    "            object_points = []\n",
    "            for i in [5, 175, 342, 156]: # [5, 175, 342, 156, 287, 57]\n",
    "                image_points.append(np.array([face_landmarks.landmark[i].x * frame.shape[1],\n",
    "                                                face_landmarks.landmark[i].y * frame.shape[0]]))\n",
    "                object_points.append(np.array([face_landmarks.landmark[i].x ,face_landmarks.landmark[i].y , face_landmarks.landmark[i].z]))\n",
    "            \n",
    "            image_points = np.array(image_points, dtype=np.float32)\n",
    "            object_points = np.array(object_points, dtype=np.float32)\n",
    "            _, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_P3P)\n",
    "            rotation_matrix_prev, _ = cv2.Rodrigues(rvec)\n",
    "            #translation_vector = np.array([0.4, -3.5, 8])\n",
    "            #translation_vector = np.array([tvec[0][0], tvec[1][0], tvec[2][0]])\n",
    "            #print(translation_vector)\n",
    "            image_points = []\n",
    "            object_points = []\n",
    "\n",
    "            for t in range(len(model_vertices)-2):\n",
    "                object_points.append(np.matmul(rotation_matrix_prev, model_vertices[t])) \n",
    "            \n",
    "            for i in [5, 175, 342, 156]:\n",
    "                image_points.append(np.array([face_landmarks.landmark[i].x * frame.shape[1],\n",
    "                                                face_landmarks.landmark[i].y * frame.shape[0]]))\n",
    "\n",
    "            image_points = np.array(image_points, dtype=np.float32)\n",
    "            object_points = np.array(object_points, dtype=np.float32)\n",
    "\n",
    "            _, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_P3P)\n",
    "            #rotation_matrix, _ = cv2.Rodrigues(rvec)\n",
    "            translation_vector = np.array([0.4, -2.5, 7])\n",
    "            \n",
    "            new_camera_pose = np.eye(4)\n",
    "            new_camera_pose[:3, :3] = rotation_matrix_prev\n",
    "            new_camera_pose[:3, 3] = translation_vector\n",
    "            #print(f\"new_camera_pose: {new_camera_pose}\", file=f)\n",
    "            new_camera_pose = Rotate_camera @ new_camera_pose\n",
    "            \n",
    "            scene.set_pose(camera_node, pose=new_camera_pose)\n",
    "            scene.set_pose(light_node, pose=new_camera_pose)\n",
    "            scene.set_pose(point_node, pose=new_camera_pose)\n",
    "            scene.set_pose(direction_node, pose=new_camera_pose)\n",
    "            \n",
    "        \n",
    "            # Render 3D model onto image plane\n",
    "            image, depth = renderer.render(scene, flags=pyrender.RenderFlags.RGBA | pyrender.RenderFlags.SHADOWS_DIRECTIONAL)\n",
    "            image = np.uint8((image) * 255)\n",
    "            # Overlay 3D model onto frame\n",
    "            #alpha = 0\n",
    "            mask = image[..., 3] > 0\n",
    "            #frame = cv2.resize(frame, (1080, 720))\n",
    "            max_depth = np.max(depth)\n",
    "            mask = np.logical_and(depth > 0, depth < max_depth)\n",
    "            mask = mask.astype(np.uint8) * 255\n",
    "\n",
    "            frame[np.where(mask != 0)] = image[:,:,:3][np.where(mask != 0)]\n",
    "\n",
    "        elif emotion_predictions==\"surprise\":\n",
    "            # Neutral image\n",
    "            sunglass_width = int(abs(face_landmarks.landmark[156].x * frame.shape[1]-face_landmarks.landmark[342].x * frame.shape[1])*1.3)\n",
    "            sunglass_height = int(abs(face_landmarks.landmark[5].y * frame.shape[1]-face_landmarks.landmark[175].y * frame.shape[1])*0.6)\n",
    "            sunglass_resized = cv2.resize(neutral_filter, (sunglass_width, sunglass_height), interpolation = cv2.INTER_CUBIC)\n",
    "            t_height = sunglass_height//4\n",
    "            x1, y1 = int(face_landmarks.landmark[156].x * frame.shape[1])-int((sunglass_width-abs(face_landmarks.landmark[156].x * frame.shape[1]-face_landmarks.landmark[342].x * frame.shape[1]))//2), t_height + int(face_landmarks.landmark[156].y * frame.shape[0]-sunglass_height//2)\n",
    "            x2, y2 = int(face_landmarks.landmark[156].x * frame.shape[1])-int((sunglass_width-abs(face_landmarks.landmark[156].x * frame.shape[1]-face_landmarks.landmark[342].x * frame.shape[1]))//2)+sunglass_width, t_height+ int(face_landmarks.landmark[156].y * frame.shape[0]+(sunglass_height-sunglass_height//2))\n",
    "            \n",
    "            mask = sunglass_resized[:, :, 3] > 0\n",
    "            \n",
    "            # Replace the square in the original frame with the modified square\n",
    "            frame[y1:y2, x1:x2][mask] = sunglass_resized[:, :, :3][mask]\n",
    "\n",
    "        elif emotion_predictions==\"neutral\":\n",
    "            \n",
    "            sunglass_width = int(abs(face_landmarks.landmark[156].x * frame.shape[1]-face_landmarks.landmark[342].x * frame.shape[1])*1.3)\n",
    "            sunglass_height = int((face_landmarks.landmark[5].x * frame.shape[1]-face_landmarks.landmark[156].x * frame.shape[1])/1.5)\n",
    "            sunglass_resized = cv2.resize(sunglasses, (sunglass_width, sunglass_height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "            x1, y1 = int(face_landmarks.landmark[156].x * frame.shape[1])-int((sunglass_width-abs(face_landmarks.landmark[156].x * frame.shape[1]-face_landmarks.landmark[342].x * frame.shape[1]))//2), int(face_landmarks.landmark[156].y * frame.shape[0]-sunglass_height//2)\n",
    "            x2, y2 = int(face_landmarks.landmark[156].x * frame.shape[1])-int((sunglass_width-abs(face_landmarks.landmark[156].x * frame.shape[1]-face_landmarks.landmark[342].x * frame.shape[1]))//2)+sunglass_width, int(face_landmarks.landmark[156].y * frame.shape[0]+(sunglass_height-sunglass_height//2))\n",
    "            \n",
    "            mask = sunglass_resized[:, :, 3] > 0\n",
    "            \n",
    "            # Replace the square in the original frame with the modified square\n",
    "            frame[y1:y2, x1:x2][mask] = sunglass_resized[:, :, :3][mask]\n",
    "        \n",
    "        org = (20, 40)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 2\n",
    "        color = (0, 0, 255)\n",
    "        thickness = 3\n",
    "        lineType = cv2.LINE_AA\n",
    "        cv2.putText(frame, emotion_predictions, org, font, fontScale, color, thickness, lineType)\n",
    "\n",
    "        #frame = cv2.circle(frame, (200,200), radius=2, color=(0, 0, 255), thickness=2)\n",
    "        cv2.imwrite(f\"/home/mka219/sfuhome/ar_project/output/frame{ind}.jpg\", frame)\n",
    "        cv2.imshow(\"AR Filter\", frame)\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "        ind = ind + 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "width = 1080\n",
    "height = 720\n",
    "\n",
    "frames_dir = \"/Users/manojkumar/Downloads/ar_project/output\"\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.avi', 0, 25, (width, height))\n",
    "\n",
    "# Loop through all the frames\n",
    "for i in range(len(os.listdir(frames_dir))):\n",
    "    # Load the frame\n",
    "    filename = frames_dir+f\"/frame{i}.jpg\"\n",
    "    frame = cv2.imread(filename)\n",
    "\n",
    "    # Check if the frame was loaded successfully\n",
    "    if frame is None or i>247:\n",
    "        continue\n",
    "            \n",
    "    # Write the frame to the video\n",
    "    out.write(frame)\n",
    "    \n",
    "# Release the VideoWriter object\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Using cached opencv-python-4.3.0.38.tar.gz (88.0 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python /Users/manojkumar/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/tmpsXW_sy\n",
      "       cwd: /private/var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/pip-install-3e3xk_/opencv-python\n",
      "  Complete output (22 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"/Users/manojkumar/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py\", line 280, in <module>\n",
      "      main()\n",
      "    File \"/Users/manojkumar/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py\", line 263, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"/Users/manojkumar/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py\", line 114, in get_requires_for_build_wheel\n",
      "      return hook(config_settings)\n",
      "    File \"/private/var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/pip-build-env-ypzdFo/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 146, in get_requires_for_build_wheel\n",
      "      return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "    File \"/private/var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/pip-build-env-ypzdFo/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 127, in _get_build_requires\n",
      "      self.run_setup()\n",
      "    File \"/private/var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/pip-build-env-ypzdFo/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 243, in run_setup\n",
      "      self).run_setup(setup_script=setup_script)\n",
      "    File \"/private/var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/pip-build-env-ypzdFo/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n",
      "      exec(compile(code, __file__, 'exec'), locals())\n",
      "    File \"setup.py\", line 448, in <module>\n",
      "      main()\n",
      "    File \"setup.py\", line 99, in main\n",
      "      % {\"ext\": re.escape(sysconfig.get_config_var(\"EXT_SUFFIX\"))}\n",
      "    File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py\", line 210, in escape\n",
      "      s = list(pattern)\n",
      "  TypeError: 'NoneType' object is not iterable\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python /Users/manojkumar/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /var/folders/kb/y8jk7b3d4x9fmt0v9316l8h40000gn/T/tmpsXW_sy Check the logs for full command output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
